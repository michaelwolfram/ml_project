\documentclass[twocolumn]{article}
\usepackage{ml_homework_template}
\usepackage{amsmath,amsthm,amssymb} 

% You should submit your final project by emailing it to fp@class.brml.org. The subject line must read "final project". 
% If the subject line does not match, an error email is returned; otherwise, a positive ACK is replied. Automated deadline 
% checking is implemented. The submission must be in before Feb. 8, 2014 (i.e., by Feb, 7, 23:59 CET). Please submit the 
% following:
%•	a short "paper" describing what you did, describing the data, describing the method, and your results. Please keep 
%   it succinct, with the class teachers as readership in mind. It should contain plots describing your results. This 
%   paper must be submitted in PDF format.
%•	all software necessary to reproduce your results, i.e., also the methodologies to preprocess the data.

\title{Final Project of Machine Learning}


\author{
\name{Jonas Uhrig}\\
\imat{03616049}\\
\email{jonas.uhrig@in.tum.de}
\And
\name{Michael Wolfram} \\
\imat{03616011}\\
\email{michaelwolfram@gmx.de}
}



\begin{document}
\onecolumn
\maketitle

\section*{Abstract}

In this paper we describe our final project for the Machine Learning class of 2013/14 at TUM. We use data set 4) containing information about body postures and movements and apply two similar Machine Learning algorithms that were implemented within this project. The used algorithms are called 'Ferns' and 'Random Tree' or 'Random Forest'.\\
The programming language of our choice is Matlab - some of the used approaches on Ferns are based on ideas from Mustafa \"Ozuyal et al. [1], the development of the trees and forests was only based on ideas from the lecture.



\twocolumn

\section{Approach}
% 'what you did'
We already know about Trees and Forests from the lecture - additionally we read of a Machine Learning algorithm called \textit{Ferns} [1], which was recently introduced to detect objects in images. Ferns have a very similar behaviour as Forests and, in fact, are completely replaceable - though the important difference lies in the striking evaluation speed for the classification of new samples. 

For object detection purposes, Ferns are built using binary values as feature vector which initially posed a problem for the chosen PUC-Rio Data Set containing continuous (e.g the sensor data) and categorical (e.g gender, names) features. A key point of Ferns is chance so we thought it might serve to randomly choose a threshold within the range of each feature and do a simple comparison, resulting in a new binary feature value - this approach was also used by Miron Bartosz Kursa [2], which encouraged us to continue the implementation.


\section{Data}
sitting: 50631
sittingdown: 11827
standing: 47370
standingup: 12415
walking: 43390



\section{Method}

\subsection{Forest}

\subsection{Ferest}

In this section we describe Ferns as well as \textit{Ferests}, which is a forest of Ferns [1]. We describe methods for training and testing together with the mathematical background and then consider different adjustments that have been made for testing and optimization issues.\\

\subsubsection{Mathematical Background}

Generally, what training is supposed to achieve is a distribution over given feature vectors that classifies a new sample into one of the given classes. In other words, we want to obtain the class which has the highest posterior probability over all class labels, given the specific features of a sample: $\arg\max_k P(C_k \vert f_1,f_2, \cdots ,f_N)$. Applying Bayes' rule results in the following expression: $\arg\max_k P(f_1,f_2, \cdots ,f_N \vert C_k)P(C_k)$, meaning that if we find the joint distribution over all features, we know to which class a sample belongs to - unfortunately, this is very hard to get. Instead, we make the Naive Bayes assumption of independent and identically distributed features which leads us to the first classification formula:
\begin{align*}
Class(f) \equiv \arg\max_k P(C_k) \prod_{n=1}^N P(f_n\vert C_k)
\end{align*}
The assumption that all features in our data set have nothing to do with each other is rather strong and often incorrect. Thus, this usually delivers quite accurate results and is easy and fast to learn.

The idea of a Ferest is to combine joint class-conditional distributions with a Naive assumption to a \textit{Semi-Naive} method which achieves accurate results within a good range of complexity and computational effort. Instead of assuming conditional independence of features, a Ferest $F_l$ uses $L$ Ferns of size $S$ and assumes independence between those. The new and final classification formula is then:
\begin{align*}
Class(f) \equiv \arg\max_k P(C_k) \prod_{l=1}^L P(F_l\vert C_k)
\end{align*}

\subsubsection{Training}

As described in the section above, a single Fern has to learn the conditional probability distributions for each of the given classes during the training phase, given a certain feature vector $\mathbf{f} = (f_1,f_2, \cdots, f_N)$ of size $N$.

In the original implementation, every Fern randomly selects $S$ features and does a binary test on them, resulting in a binary number between $0$ and $2^S-1$. However, the original implementation was working on image patches to detect objects and the binary tests were picked by comparing two random pixel intensities in the given patch. For our initial implementation, as Kursa [2] suggests, we chose a random threshold within the currently given feature space and compare this threshold with the respective value of each sample.

Like this, we get a decimal number for every training sample by iteratively applying the random tests - this number is then used to put the current sample into one of $2^S$ \textit{buckets}, comparable to hashing.\\
Using this pattern of dropping samples into buckets for every sample of a class present in the training set delivers a multinomial distribution (\textit{histogram}) of the new features (random binary tests) for the regarded class. This has to be normalized prior to be an actual probability distribution.

To train a Ferest, we simply train $L$ Ferns and store the specific histograms together with the randomly picked thresholds for the tests.

\subsubsection{Testing}

The classification of a new sample using a single Fern is very efficient: The previously stored tests are applied to the tested sample in the same order to find the bucket into which this sample drops. The combined buckets of all classes, after normalizing, then represent the corresponding class posterior distribution, given the features of the tested sample. This means that every Fern knows with which certainty a sample would be of which class.

For a Ferest, the results of all Ferns are combined using the Naive Bayes' assumption - which is now more appropriate as all Ferns check on random features and thresholds.

\subsubsection{Adjustments}

Trying to achieve more accurate results, 


\section{Results}

\subsection{Forest}

\subsection{Fererst}

\subsection{Comparison}


\onecolumn
\section*{References}

\begin{tabular}{p{1cm}p{11cm}}

$[1]$ & M. \"Ozuysal, P. Fua, and V. Lepetit. Fast Keypoint Recognition in Ten Lines of Code. \textit{2007 IEEE Conference on Computer Vision and Pattern Recognition}, pages 1-8, June 2007\\
 & \\
$[2]$ & M. B. Kursa. Random ferns method implementation for the general-purpose machine learning. \textit{Interdisciplinary Centre for Mathematical and Computational Modelling, University of Warsaw}, February 2012

\end{tabular}



\end{document}
